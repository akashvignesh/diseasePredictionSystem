{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating profile report...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff75a34ca9d4bb6b16c2a7e928a231b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7331f979594b409791ea5772c069c868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677e09873f6841059f51054334d37d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from ydata_profiling import ProfileReport\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "%matplotlib inline\n",
    "# Load dataset\n",
    "beforeDataClean = pd.read_csv(\"D:\\\\Code\\\\ESDS_503\\\\GIT\\\\diseasePredictionSystem\\\\diabetes.csv\")\n",
    "# Generate Profile Report\n",
    "print(\"Generating profile report...\")\n",
    "profile = ProfileReport(beforeDataClean, title=\"Diabetes Dataset Profiling Report\")\n",
    "profile.to_notebook_iframe()\n",
    "# Compute the correlation matrix for numerical columns\n",
    "numerical_columns = beforeDataClean.select_dtypes(include=['number'])\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()\n",
    "# Data Cleaning\n",
    "afterDataClean = beforeDataClean.copy(deep=True)\n",
    "afterDataClean[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = (\n",
    "    afterDataClean[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN))\n",
    "afterDataClean['Glucose'] = afterDataClean['Glucose'].fillna(round(afterDataClean['Glucose'].mean()))\n",
    "afterDataClean['BloodPressure'] = afterDataClean['BloodPressure'].fillna(round(afterDataClean['BloodPressure'].mean()))\n",
    "afterDataClean['SkinThickness'] = afterDataClean['SkinThickness'].fillna(round(afterDataClean['SkinThickness'].median()))\n",
    "afterDataClean['Insulin'] = afterDataClean['Insulin'].fillna(round(afterDataClean['Insulin'].median()))\n",
    "afterDataClean['BMI'] = afterDataClean['BMI'].fillna(round(afterDataClean['BMI'].median()))\n",
    "\n",
    "# Check for remaining NaN values\n",
    "print(\"Remaining NaN values:\", afterDataClean.isnull().sum())\n",
    "\n",
    "\n",
    "# Final data for processing\n",
    "data = afterDataClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n",
      "   PatientID  Pregnancies  Age  Glucose  BloodPressure  SkinThickness  \\\n",
      "0          1            6   50      148             72             35   \n",
      "1          2            1   31       85             66             29   \n",
      "2          3            8   32      183             64             29   \n",
      "3          4            1   21       89             66             23   \n",
      "4          5            0   33      137             40             35   \n",
      "\n",
      "   Insulin   BMI  DiabetesPedigreeFunction  Outcome  \n",
      "0      125  33.6                     0.627        1  \n",
      "1      125  26.6                     0.351        0  \n",
      "2      125  23.3                     0.672        1  \n",
      "3       94  28.1                     0.167        0  \n",
      "4      168  43.1                     2.288        1  \n",
      "        PatientID  Pregnancies         Age     Glucose  BloodPressure  \\\n",
      "count  768.000000   768.000000  768.000000  768.000000     768.000000   \n",
      "mean   384.500000     3.845052   33.240885  121.688802      72.386719   \n",
      "std    221.846794     3.369578   11.760232   30.435959      12.096642   \n",
      "min      1.000000     0.000000   21.000000   44.000000      24.000000   \n",
      "25%    192.750000     1.000000   24.000000   99.750000      64.000000   \n",
      "50%    384.500000     3.000000   29.000000  117.000000      72.000000   \n",
      "75%    576.250000     6.000000   41.000000  140.250000      80.000000   \n",
      "max    768.000000    17.000000   81.000000  199.000000     122.000000   \n",
      "\n",
      "       SkinThickness     Insulin         BMI  DiabetesPedigreeFunction  \\\n",
      "count     768.000000  768.000000  768.000000                768.000000   \n",
      "mean       29.108073  140.671875   32.450911                  0.471876   \n",
      "std         8.791221   86.383060    6.875366                  0.331329   \n",
      "min         7.000000   14.000000   18.200000                  0.078000   \n",
      "25%        25.000000  121.500000   27.500000                  0.243750   \n",
      "50%        29.000000  125.000000   32.000000                  0.372500   \n",
      "75%        32.000000  127.250000   36.600000                  0.626250   \n",
      "max        99.000000  846.000000   67.100000                  2.420000   \n",
      "\n",
      "          Outcome  \n",
      "count  768.000000  \n",
      "mean     0.348958  \n",
      "std      0.476951  \n",
      "min      0.000000  \n",
      "25%      0.000000  \n",
      "50%      0.000000  \n",
      "75%      1.000000  \n",
      "max      1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Database Connection Functions\n",
    "def create_connection(db_file, delete_db=False):\n",
    "    if delete_db and os.path.exists(db_file):\n",
    "        os.remove(db_file)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        conn.execute(\"PRAGMA foreign_keys = 1\")\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "# Create database and tables\n",
    "dbConnection = create_connection(\"normalizedData.db\", True)\n",
    "\n",
    "patientTable = '''CREATE TABLE Patient (\n",
    "    PatientID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Pregnancies INTEGER,\n",
    "    Age INT\n",
    ");'''\n",
    "healthMatrixTable = '''CREATE TABLE HealthMetrics (\n",
    "    PatientID INTEGER,\n",
    "    Glucose INTEGER,\n",
    "    BloodPressure INTEGER,\n",
    "    SkinThickness INTEGER,\n",
    "    Insulin INTEGER,\n",
    "    BMI FLOAT,\n",
    "    DiabetesPedigreeFunction FLOAT,\n",
    "    FOREIGN KEY (PatientID) REFERENCES Patient(PatientID)\n",
    ");'''\n",
    "diseaseOutcomeTable = '''CREATE TABLE Outcome (\n",
    "    PatientID INTEGER,\n",
    "    DiabetesStatus INTEGER,\n",
    "    FOREIGN KEY (PatientID) REFERENCES Patient(PatientID)\n",
    ");'''\n",
    "\n",
    "create_table(dbConnection, patientTable)\n",
    "create_table(dbConnection, healthMatrixTable)\n",
    "create_table(dbConnection, diseaseOutcomeTable)\n",
    "\n",
    "# Insert data into tables\n",
    "def insert_patient_data(conn, data):\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in data.iterrows():\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO Patient (Pregnancies, Age) VALUES (?, ?)\",\n",
    "            (row[\"Pregnancies\"], row[\"Age\"])\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def insert_health_metrics(conn, data):\n",
    "    cursor = conn.cursor()\n",
    "    patient_ids = cursor.execute(\"SELECT PatientID FROM Patient\").fetchall()\n",
    "    for _, (row, patient_id) in enumerate(zip(data.iterrows(), patient_ids)):\n",
    "        row = row[1]\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO HealthMetrics (\n",
    "                PatientID, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                patient_id[0],\n",
    "                row[\"Glucose\"],\n",
    "                row[\"BloodPressure\"],\n",
    "                row[\"SkinThickness\"],\n",
    "                row[\"Insulin\"],\n",
    "                row[\"BMI\"],\n",
    "                row[\"DiabetesPedigreeFunction\"]\n",
    "            )\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def insert_outcome(conn, data):\n",
    "    cursor = conn.cursor()\n",
    "    patient_ids = cursor.execute(\"SELECT PatientID FROM Patient\").fetchall()\n",
    "    for _, (row, patient_id) in enumerate(zip(data.iterrows(), patient_ids)):\n",
    "        row = row[1]\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO Outcome (PatientID, DiabetesStatus) VALUES (?, ?)\",\n",
    "            (patient_id[0], row[\"Outcome\"])\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "insert_patient_data(dbConnection, data)\n",
    "insert_health_metrics(dbConnection, data)\n",
    "insert_outcome(dbConnection, data)\n",
    "print(\"Data inserted successfully!\")\n",
    "\n",
    "# Load data from database\n",
    "dataFromTable = '''SELECT \n",
    "    p.PatientID, \n",
    "    p.Pregnancies, \n",
    "    p.Age, \n",
    "    hm.Glucose, \n",
    "    hm.BloodPressure, \n",
    "    hm.SkinThickness, \n",
    "    hm.Insulin, \n",
    "    hm.BMI, \n",
    "    hm.DiabetesPedigreeFunction, \n",
    "    o.DiabetesStatus AS Outcome\n",
    "FROM Patient p\n",
    "JOIN HealthMetrics hm ON p.PatientID = hm.PatientID\n",
    "JOIN Outcome o ON p.PatientID = o.PatientID;'''\n",
    "data = pd.read_sql_query(dataFromTable, dbConnection)\n",
    "print(data.head())\n",
    "dbConnection.close()\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing -- Feature Scaling \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPregnancies\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Define transformations for numerical data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m numerical_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(np\u001b[38;5;241m.\u001b[39mlog1p, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),  \u001b[38;5;66;03m# Log Transformation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler())  \u001b[38;5;66;03m# Standard Scaling\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define transformations for categorical data with OneHotEncoder's sparse_output set to False\u001b[39;00m\n\u001b[0;32m     16\u001b[0m categorical_transformer \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     17\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m'\u001b[39m, OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)),  \u001b[38;5;66;03m# One-Hot Encoding with dense output\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m'\u001b[39m, MinMaxScaler())  \u001b[38;5;66;03m# MinMax Scaling\u001b[39;00m\n\u001b[0;32m     19\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Splitting features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "categorical_features = ['Pregnancies']\n",
    "\n",
    "# Define transformations for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('log', FunctionTransformer(np.log1p, validate=True)),  # Log Transformation\n",
    "    ('scaler', StandardScaler())  # Standard Scaling\n",
    "])\n",
    "\n",
    "# Define transformations for categorical data with OneHotEncoder's sparse_output set to False\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),  # One-Hot Encoding with dense output\n",
    "    ('minmax', MinMaxScaler())  # MinMax Scaling\n",
    "])\n",
    "\n",
    "# Combine preprocessors using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Apply preprocessing and SMOTE\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_transformed, y)\n",
    "\n",
    "\n",
    "# Class distribution profiling after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(y_res.value_counts())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy  F1 Score      Confusion Matrix  \\\n",
      "Logistic Regression    0.735  0.738916  [[72, 27], [26, 75]]   \n",
      "Random Forest           0.81  0.819048  [[76, 23], [15, 86]]   \n",
      "SVM                     0.76  0.773585  [[70, 29], [19, 82]]   \n",
      "Gradient Boosting       0.78  0.786408  [[75, 24], [20, 81]]   \n",
      "Stacking               0.805  0.815166  [[75, 24], [15, 86]]   \n",
      "\n",
      "                                                 Classification Report  \n",
      "Logistic Regression                precision    recall  f1-score   ...  \n",
      "Random Forest                      precision    recall  f1-score   ...  \n",
      "SVM                                precision    recall  f1-score   ...  \n",
      "Gradient Boosting                  precision    recall  f1-score   ...  \n",
      "Stacking                           precision    recall  f1-score   ...  \n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "log_reg = LogisticRegression(max_iter=200, random_state=42)\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Random Forest Hyperparameter Tuning\n",
    "param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Stacking\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[('lr', log_reg), ('rf', best_rf), ('svm', svm)],\n",
    "    final_estimator=LogisticRegression(max_iter=200, random_state=42)\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "models = {'Logistic Regression': log_reg, 'Random Forest': best_rf, 'SVM': svm, 'Gradient Boosting': gradient_boosting, 'Stacking': stacking}\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Confusion Matrix': confusion_matrix(y_test, y_pred),\n",
    "        'Classification Report': classification_report(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved: Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Save best model\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "joblib.dump(models[best_model_name], 'disease_prediction_model.pkl')\n",
    "print(f\"Best model saved: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d5b36f2bce418f8ac9d18c131e61a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Logistic Regression' already exists. Creating a new version of this model...\n",
      "2024/12/18 21:02:53 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Logistic Regression, version 10\n",
      "Created version '10' of model 'Logistic Regression'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run amusing-hen-501 at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1/runs/fd549fef2fa74245a5ab6b20d51be5fa\n",
      "ðŸ§ª View experiment at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3a08e7ef0c4107b8731a6dbff3b77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Random Forest' already exists. Creating a new version of this model...\n",
      "2024/12/18 21:03:11 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Random Forest, version 10\n",
      "Created version '10' of model 'Random Forest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run polite-stag-555 at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1/runs/0fe9cdc09a594ee0941eea5b61e141a8\n",
      "ðŸ§ª View experiment at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6710b26fdeb340738dc3261ba8769a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'SVM' already exists. Creating a new version of this model...\n",
      "2024/12/18 21:03:29 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: SVM, version 10\n",
      "Created version '10' of model 'SVM'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run rumbling-panda-629 at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1/runs/1ac6c4b0258c4f23abdee5d62a2a75d0\n",
      "ðŸ§ª View experiment at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0370bd0ac974a469cef2ce82dcc6b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Gradient Boosting' already exists. Creating a new version of this model...\n",
      "2024/12/18 21:03:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Gradient Boosting, version 10\n",
      "Created version '10' of model 'Gradient Boosting'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run bald-quail-729 at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1/runs/01fbb6719e5e4505b381c8c85e6ab3d2\n",
      "ðŸ§ª View experiment at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cfa2a00a3d4022863d5129ef62fe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Stacking' already exists. Creating a new version of this model...\n",
      "2024/12/18 21:04:07 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Stacking, version 10\n",
      "Created version '10' of model 'Stacking'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run polite-pig-474 at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1/runs/6de7d058469a4223b936f93a78ec2464\n",
      "ðŸ§ª View experiment at: https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MLFLOW_TRACKING_URI=\"https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME']='s3akash'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD']='4d779774d517aa973e2858c5d93af0e87a4ca457'\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"diseasePredictionSystemModel\")\n",
    "\n",
    "# Start an MLflow run\n",
    "for algo, model in models.items():\n",
    "    with mlflow.start_run():\n",
    "        if isinstance(model, GridSearchCV):\n",
    "            score = model.best_score_\n",
    "            params = model.best_params_\n",
    "            best_model = model.best_estimator_\n",
    "        else:\n",
    "            score = accuracy_score(y_test, model.predict(X_test))\n",
    "            params = {}\n",
    "            best_model = model\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"accuracy\", score)\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, model.predict(X_test)))\n",
    "        \n",
    "        # Infer the model signature\n",
    "        signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=best_model,\n",
    "            artifact_path=\"model\",\n",
    "            signature=signature,\n",
    "            input_example=X_train,\n",
    "            registered_model_name=algo,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
